<html><!-- Mirrored from developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 14 Feb 2023 04:49:12 GMT --><!-- Added by HTTrack --><head><meta http-equiv="content-type" content="text/html;charset=utf-8"><!-- /Added by HTTrack -->
<meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="../../../../../favicon-48x48.cbbd161b.png"><link rel="apple-touch-icon" href="../../../../../apple-touch-icon.6803c6f0.png"><meta name="theme-color" content="#ffffff"><link rel="manifest" href="../../../../../manifest.56b1cedc.json"><link rel="search" type="application/opensearchdescription+xml" href="https://developer.mozilla.org/opensearch.xml" title="MDN Web Docs"><title>Basic concepts behind Web Audio API</title><link rel="alternate" title="Les concepts de base de la Web Audio API" href="https://developer.mozilla.org/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="fr"><link rel="alternate" title="Basic concepts behind Web Audio API" href="https://developer.mozilla.org/ja/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="ja"><link rel="alternate" title="Web Audio API의 기본 개념" href="https://developer.mozilla.org/ko/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="ko"><link rel="alternate" title="网页音频接口的基本概念" href="https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="zh"><link rel="alternate" title="Basic concepts behind Web Audio API" href="Basic_concepts_behind_Web_Audio_API.html" hreflang="en"><meta name="robots" content="index, follow"><meta name="description" content="This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does."><meta property="og:url" content="Basic_concepts_behind_Web_Audio_API.html"><meta property="og:title" content="Basic concepts behind Web Audio API - Web APIs | MDN"><meta property="og:locale" content="en-US"><meta property="og:description" content="This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does."><meta property="og:image" content="../../../../../mdn-social-share.cd6c4a5a.png"><meta property="twitter:card" content="summary_large_image"><link rel="canonical" href="Basic_concepts_behind_Web_Audio_API.html"><style media="print">.article-actions-container,.document-toc-container,.language-menu,.main-menu-toggle,.mdn-cta-container,.on-github,.page-footer,.sidebar,.top-navigation-main,ul.prev-next{display:none!important}.main-page-content,.main-page-content pre{padding:2px}.main-page-content pre{border-left-width:2px}</style><script src="../../../../../static/js/ga.js" defer=""></script><script defer="defer" src="../../../../../static/js/main.5c9d40d0.js"></script><link href="../../../../../static/css/main.7d378400.css" rel="stylesheet"></head><body><script>document.body.addEventListener("load",(t=>{t.target.classList.contains("interactive")&&t.target.setAttribute("data-readystate","complete")}),{capture:!0});const c={light:"#ffffff",dark:"#1b1b1b"};if(window&&document.documentElement)try{const t=window.localStorage.getItem("theme");t&&(document.documentElement.className=t,document.documentElement.style.backgroundColor=c[t])}catch(t){console.warn("Unable to read theme from localStorage",t)}</script><div id="root"><ul id="nav-access" class="a11y-nav"><li><a id="skip-main" href="#content">Skip to main content</a></li><li><a id="skip-search" href="#top-nav-search-input">Skip to search</a></li><li><a id="skip-select-language" href="#languages-switcher-button">Skip to select language</a></li></ul><div class="page-wrapper  category-api document-page"><div class="main-document-header-container"><header class="main-document-header-container top-navigation 
      
      "><div class="container "><div class="top-navigation-wrap"><a href="https://developer.mozilla.org/en-US/" class="logo" aria-label="MDN homepage"><svg id="mdn-docs-logo" xmlns="http://www.w3.org/2000/svg" x="0" y="0" viewBox="0 0 694.9 104.4" style="enable-background:new 0 0 694.9 104.4" xml:space="preserve" role="img"><title>MDN Web Docs</title><style>.logo-m{fill:var(--text-link)}</style><g class="logo-m"><path d="M40.3 0 11.7 92.1H0L28.5 0h11.8zM50.7 0v92.1H40.3V0h10.4zM91 0 62.5 92.1H50.8L79.3 0H91zM101.4 0v92.1H91V0h10.4z"></path></g><path class="logo-m" d="M627.9 95.6h67v8.8h-67v-8.8z"></path><g style="fill:var(--text-primary)"><path d="M367 42h-4l-10.7 30.8h-5.5l-10.8-26h-.4l-10.5 26h-5.2L308.7 42h-3.8v-5.6H323V42h-6.5l6.8 20.4h.4l10.3-26h4.7l11.2 26h.5l5.7-20.3h-6.2v-5.6H367V42zM401.9 62c-.4 3.2-2 5.9-4.7 8.2-2.8 2.3-6.5 3.4-11.3 3.4-5.4 0-9.7-1.6-13.1-4.7-3.3-3.2-5-7.7-5-13.7 0-5.7 1.6-10.3 4.7-14s7.4-5.5 12.9-5.5c5.1 0 9.1 1.6 11.9 4.7s4.3 6.9 4.3 11.3c0 1.5-.2 3-.5 4.7h-25.6c.3 7.7 4 11.6 10.9 11.6 2.9 0 5.1-.7 6.5-2 1.5-1.4 2.5-3 3-4.9l6 .9zM394 51.3c.2-2.4-.4-4.7-1.8-6.9s-3.8-3.3-7-3.3c-3.1 0-5.3 1-6.9 3-1.5 2-2.5 4.4-2.8 7.2H394zM445 53.7c0 5-1.3 9.5-4 13.7s-6.9 6.2-12.7 6.2c-6 0-10.3-2.2-12.7-6.7-.1.4-.2 1.4-.4 2.9s-.3 2.5-.4 2.9h-7.3c.3-1.7.6-3.5.8-5.3.3-1.8.4-3.7.4-5.5V22.3h-6v-5.6H416v27c1.1-2.2 2.7-4.1 4.7-5.7 2-1.6 4.8-2.4 8.4-2.4 4.6 0 8.4 1.6 11.4 4.7 3 3.2 4.5 7.6 4.5 13.4zm-7.7.6c0-4.2-1-7.4-3-9.5-2-2.2-4.4-3.3-7.4-3.3-3.4 0-6 1.2-8 3.7-1.9 2.4-2.9 5-3 7.7V57c0 3 1 5.6 3 7.7s4.5 3.1 7.6 3.1c3.6 0 6.3-1.3 8.1-3.9 1.8-2.7 2.7-5.9 2.7-9.6zM506.5 72.8h-13.2v-7.2c-1.2 2.2-2.8 4.1-4.9 5.6-2.1 1.6-4.8 2.4-8.3 2.4-4.8 0-8.7-1.6-11.6-4.9-2.9-3.2-4.3-7.7-4.3-13.3 0-5 1.3-9.6 4-13.7 2.6-4.1 6.9-6.2 12.8-6.2 5.7 0 9.8 2.2 12.3 6.5V22.3h-8.6v-5.6h15.8v50.6h6v5.5zM493.2 56v-4.4c-.1-3-1.2-5.5-3.2-7.3s-4.4-2.8-7.2-2.8c-3.6 0-6.3 1.3-8.2 3.9-1.9 2.6-2.8 5.8-2.8 9.6 0 4.1 1 7.3 3 9.5s4.5 3.3 7.4 3.3c3.2 0 5.8-1.3 7.8-3.8 2.1-2.6 3.1-5.3 3.2-8zM546.3 54.6c0 5.6-1.8 10.2-5.3 13.7s-8.2 5.3-13.9 5.3-10.1-1.7-13.4-5.1c-3.3-3.4-5-7.9-5-13.5 0-5.3 1.6-9.9 4.7-13.7 3.2-3.8 7.9-5.7 14.2-5.7s11 1.9 14.1 5.7c3 3.7 4.6 8.1 4.6 13.3zm-7.7-.2c0-4-1-7.2-3-9.5s-4.8-3.5-8.2-3.5c-3.6 0-6.4 1.2-8.3 3.7s-2.9 5.6-2.9 9.5c0 3.7.9 6.8 2.8 9.4 1.9 2.6 4.6 3.9 8.3 3.9 3.6 0 6.4-1.3 8.4-3.8 1.9-2.6 2.9-5.8 2.9-9.7zM583.6 60.2c-.4 3.2-1.9 6.3-4.4 9.1-2.5 2.9-6.4 4.3-11.8 4.3-5.2 0-9.4-1.6-12.6-4.8-3.2-3.2-4.8-7.7-4.8-13.7 0-5.5 1.6-10.1 4.7-13.9 3.2-3.8 7.6-5.7 13.2-5.7 2.3 0 4.6.3 6.7.8 2.2.5 4.2 1.5 6.2 2.9l1.5 9.5-5.9.7-1.3-6.1c-2.1-1.2-4.5-1.8-7.2-1.8-3.5 0-6.1 1.2-7.7 3.7-1.7 2.5-2.5 5.7-2.5 9.6 0 4.1.9 7.3 2.7 9.5 1.8 2.3 4.4 3.4 7.8 3.4 5.2 0 8.2-2.9 9.2-8.8l6.2 1.3zM618.3 62.1c0 3.6-1.5 6.5-4.6 8.5s-7 3-11.7 3c-5.7 0-10.6-1.2-14.6-3.6l1.2-8.8 5.7.6-.2 4.7c1.1.5 2.3.9 3.6 1.1s2.6.3 3.9.3c2.4 0 4.5-.4 6.5-1.3 1.9-.9 2.9-2.2 2.9-4.1 0-1.8-.8-3.1-2.3-3.8s-3.5-1.3-5.8-1.7-4.6-.9-6.9-1.4c-2.3-.6-4.2-1.6-5.7-2.9-1.6-1.4-2.3-3.5-2.3-6.3 0-4.1 1.5-6.9 4.6-8.5s6.4-2.4 9.9-2.4c2.6 0 5 .3 7.2.9 2.2.6 4.3 1.4 6.1 2.4l.8 8.8-5.8.7-.8-5.7c-2.3-1-4.7-1.6-7.2-1.6-2.1 0-3.7.4-5.1 1.1-1.3.8-2 2-2 3.8 0 1.7.8 2.9 2.3 3.6 1.5.7 3.4 1.2 5.7 1.6 2.2.4 4.5.8 6.7 1.4 2.2.6 4.1 1.6 5.7 3 1.4 1.6 2.2 3.7 2.2 6.6zM197.6 73.2h-17.1v-5.5h3.8V51.9c0-3.7-.7-6.3-2.1-7.9-1.4-1.6-3.3-2.3-5.7-2.3-3.2 0-5.6 1.1-7.2 3.4s-2.4 4.6-2.5 6.9v15.6h6v5.5h-17.1v-5.5h3.8V51.9c0-3.8-.7-6.4-2.1-7.9-1.4-1.5-3.3-2.3-5.6-2.3-3.2 0-5.5 1.1-7.2 3.3-1.6 2.2-2.4 4.5-2.5 6.9v15.8h6.9v5.5h-20.2v-5.5h6V42.4h-6.1v-5.6h13.4v6.4c1.2-2.1 2.7-3.8 4.7-5.2 2-1.3 4.4-2 7.3-2s5.3.7 7.5 2.1c2.2 1.4 3.7 3.5 4.5 6.4 1.1-2.5 2.7-4.5 4.9-6.1s4.8-2.4 7.9-2.4c3.5 0 6.5 1.1 8.9 3.3s3.7 5.6 3.7 10.2v18.2h6.1v5.5zm42.5 0h-13.2V66c-1.2 2.2-2.8 4.1-4.9 5.6-2.1 1.6-4.8 2.4-8.3 2.4-4.8 0-8.7-1.6-11.6-4.9-2.9-3.2-4.3-7.7-4.3-13.3 0-5 1.3-9.6 4-13.7 2.6-4.1 6.9-6.2 12.8-6.2s9.8 2.2 12.3 6.5V22.7h-8.6v-5.6h15.8v50.6h6v5.5zm-13.3-16.8V52c-.1-3-1.2-5.5-3.2-7.3s-4.4-2.8-7.2-2.8c-3.6 0-6.3 1.3-8.2 3.9-1.9 2.6-2.8 5.8-2.8 9.6 0 4.1 1 7.3 3 9.5s4.5 3.3 7.4 3.3c3.2 0 5.8-1.3 7.8-3.8 2.1-2.6 3.1-5.3 3.2-8zm61.5 16.8H269v-5.5h6V51.9c0-3.7-.7-6.3-2.2-7.9-1.4-1.6-3.4-2.3-5.7-2.3-3.1 0-5.6 1-7.4 3s-2.8 4.4-2.9 7v15.9h6v5.5h-19.3v-5.5h6V42.4h-6.2v-5.6h13.6V43c2.6-4.6 6.8-6.9 12.7-6.9 3.6 0 6.7 1.1 9.2 3.3s3.7 5.6 3.7 10.2v18.2h6v5.4h-.2z"></path></g></svg></a><button title="Open main menu" type="button" class="button action has-icon main-menu-toggle" aria-haspopup="menu" aria-label="Open main menu" aria-expanded="false"><span class="button-wrap"><span class="icon icon-menu "></span><span class="visually-hidden">Open main menu</span></span></button></div><div class="top-navigation-main"><nav class="main-nav" aria-label="Main menu"><ul class="main-menu nojs"><li class="top-level-entry-container"><button type="button" id="references-button" class="top-level-entry menu-toggle" aria-controls="references-menu" aria-expanded="false">References</button><a href="https://developer.mozilla.org/en-US/docs/Web" class="top-level-entry">References</a><ul id="references-menu" class="submenu references hidden inline-submenu-lg" aria-labelledby="references-button"><li class="apis-link-container mobile-only "><a href="https://developer.mozilla.org/en-US/docs/Web" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Overview / Web Technology</div><p class="submenu-item-description">Web technology reference for developers</p></div></a></li><li class="html-link-container "><a href="https://developer.mozilla.org/en-US/docs/Web/HTML" class="submenu-item "><div class="submenu-icon html"></div><div class="submenu-content-container"><div class="submenu-item-heading">HTML</div><p class="submenu-item-description">Structure of content on the web</p></div></a></li><li class="css-link-container "><a href="https://developer.mozilla.org/en-US/docs/Web/CSS" class="submenu-item "><div class="submenu-icon css"></div><div class="submenu-content-container"><div class="submenu-item-heading">CSS</div><p class="submenu-item-description">Code used to describe document style</p></div></a></li><li class="javascript-link-container "><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" class="submenu-item "><div class="submenu-icon javascript"></div><div class="submenu-content-container"><div class="submenu-item-heading">JavaScript</div><p class="submenu-item-description">General-purpose scripting language</p></div></a></li><li class="http-link-container "><a href="https://developer.mozilla.org/en-US/docs/Web/HTTP" class="submenu-item "><div class="submenu-icon http"></div><div class="submenu-content-container"><div class="submenu-item-heading">HTTP</div><p class="submenu-item-description">Protocol for transmitting web resources</p></div></a></li><li class="apis-link-container "><a href="../../API.html" class="submenu-item "><div class="submenu-icon apis"></div><div class="submenu-content-container"><div class="submenu-item-heading">Web APIs</div><p class="submenu-item-description">Interfaces for building web applications</p></div></a></li><li class="apis-link-container "><a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Web Extensions</div><p class="submenu-item-description">Developing extensions for web browsers</p></div></a></li><li class="apis-link-container desktop-only "><a href="https://developer.mozilla.org/en-US/docs/Web" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Web Technology</div><p class="submenu-item-description">Web technology reference for developers</p></div></a></li></ul></li><li class="top-level-entry-container"><button type="button" id="guides-button" class="top-level-entry menu-toggle" aria-controls="guides-menu" aria-expanded="false">Guides</button><a href="https://developer.mozilla.org/en-US/docs/Learn" class="top-level-entry">Guides</a><ul id="guides-menu" class="submenu guides hidden inline-submenu-lg" aria-labelledby="guides-button"><li class="apis-link-container mobile-only "><a href="https://developer.mozilla.org/en-US/docs/Learn" class="submenu-item "><div class="submenu-icon learn"></div><div class="submenu-content-container"><div class="submenu-item-heading">Overview / MDN Learning Area</div><p class="submenu-item-description">Learn web development</p></div></a></li><li class="apis-link-container desktop-only "><a href="https://developer.mozilla.org/en-US/docs/Learn" class="submenu-item "><div class="submenu-icon learn"></div><div class="submenu-content-container"><div class="submenu-item-heading">MDN Learning Area</div><p class="submenu-item-description">Learn web development</p></div></a></li><li class="html-link-container "><a href="https://developer.mozilla.org/en-US/docs/Learn/HTML" class="submenu-item "><div class="submenu-icon html"></div><div class="submenu-content-container"><div class="submenu-item-heading">HTML</div><p class="submenu-item-description">Learn to structure web content with HTML</p></div></a></li><li class="css-link-container "><a href="https://developer.mozilla.org/en-US/docs/Learn/CSS" class="submenu-item "><div class="submenu-icon css"></div><div class="submenu-content-container"><div class="submenu-item-heading">CSS</div><p class="submenu-item-description">Learn to style content using CSS</p></div></a></li><li class="javascript-link-container "><a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript" class="submenu-item "><div class="submenu-icon javascript"></div><div class="submenu-content-container"><div class="submenu-item-heading">JavaScript</div><p class="submenu-item-description">Learn to run scripts in the browser</p></div></a></li><li class=" "><a href="https://developer.mozilla.org/en-US/docs/Web/Accessibility" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Accessibility</div><p class="submenu-item-description">Learn to make the web accessible to all</p></div></a></li></ul></li><li class="top-level-entry-container"><button type="button" id="mdn-plus-button" class="top-level-entry menu-toggle" aria-controls="mdn-plus-menu" aria-expanded="false">MDN Plus</button><a href="https://developer.mozilla.org/en-US/plus" class="top-level-entry">MDN Plus</a><ul id="mdn-plus-menu" class="submenu mdn-plus hidden inline-submenu-lg" aria-labelledby="mdn-plus-button"><li class=" "><a href="https://developer.mozilla.org/en-US/plus" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Overview</div><p class="submenu-item-description">A customized MDN experience</p></div></a></li><li class=" "><a href="https://developer.mozilla.org/en-US/plus/updates" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Updates</div><p class="submenu-item-description">All browser compatibility updates at a glance</p></div></a></li><li class=" "><a href="https://developer.mozilla.org/en-US/plus/docs/features/overview" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">Documentation</div><p class="submenu-item-description">Learn how to use MDN Plus</p></div></a></li><li class=" "><a href="https://developer.mozilla.org/en-US/plus/docs/faq" class="submenu-item "><div class="submenu-icon"></div><div class="submenu-content-container"><div class="submenu-item-heading">FAQ</div><p class="submenu-item-description">Frequently asked questions about MDN Plus</p></div></a></li></ul></li></ul></nav><div class="header-search"><form action="https://developer.mozilla.org/en-US/search" role="search" aria-haspopup="listbox" aria-owns="top-nav-search-menu" aria-expanded="false" class="search-form search-widget" id="top-nav-search-form"><label id="top-nav-search-label" for="top-nav-search-input" class="visually-hidden">Search MDN</label><input id="top-nav-search-input" aria-autocomplete="list" aria-controls="top-nav-search-menu" aria-labelledby="top-nav-search-label" autocomplete="off" type="search" class="search-input-field" name="q" placeholder="   " required="" value=""><button type="button" class="button action has-icon clear-search-button"><span class="button-wrap"><span class="icon icon-cancel "></span><span class="visually-hidden">Clear search input</span></span></button><button type="submit" class="button action has-icon search-button"><span class="button-wrap"><span class="icon icon-search "></span><span class="visually-hidden">Search</span></span></button><div id="top-nav-search-menu" role="listbox" aria-labelledby="top-nav-search-label"></div></form></div><div class="theme-switcher-menu"><button type="button" class="button action has-icon theme-switcher-menu small" aria-haspopup="menu"><span class="button-wrap"><span class="icon icon-theme-os-default "></span>Theme</span></button></div><ul class="auth-container"><li><a href="https://developer.mozilla.org/users/fxa/login/authenticate/?next=%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API%2FBasic_concepts_behind_Web_Audio_API" class="signin-link" rel="nofollow">Already a subscriber?</a></li><li><a class="button primary mdn-plus-subscribe-link" href="https://developer.mozilla.org/en-US/plus"><span class="button-wrap">Get MDN Plus</span></a></li></ul></div></div></header><div class="article-actions-container"><div class="container"><button type="button" class="button action has-icon sidebar-button" aria-label="Expand sidebar" aria-expanded="false" aria-controls="sidebar-quicklinks"><span class="button-wrap"><span class="icon icon-sidebar "></span></span></button><nav class="breadcrumbs-container" aria-label="Breadcrumb"><ol typeof="BreadcrumbList" vocab="https://schema.org/" aria-label="breadcrumbs"><li property="itemListElement" typeof="ListItem"><a class="breadcrumb" property="item" typeof="WebPage" href="https://developer.mozilla.org/en-US/docs/Web"><span property="name">References</span></a><meta property="position" content="1"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb" property="item" typeof="WebPage" href="../../API.html"><span property="name">Web APIs</span></a><meta property="position" content="2"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb" property="item" typeof="WebPage" href="../Web_Audio_API.html"><span property="name">Web Audio API</span></a><meta property="position" content="3"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb-current-page" property="item" typeof="WebPage" href="Basic_concepts_behind_Web_Audio_API.html"><span property="name">Basic concepts behind Web Audio API</span></a><meta property="position" content="4"></li></ol></nav><div class="article-actions"><button type="button" class="button action has-icon article-actions-toggle" aria-label="Article actions"><span class="button-wrap"><span class="icon icon-ellipses "></span><span class="article-actions-dialog-heading">Article Actions</span></span></button><ul class="article-actions-entries"><li class="article-actions-entry"><div class="languages-switcher-menu open-on-focus-within"><button id="languages-switcher-button" type="button" class="button action small has-icon languages-switcher-menu" aria-haspopup="menu"><span class="button-wrap"><span class="icon icon-language "></span>English (US)</span></button></div></li></ul></div></div></div></div><div class="main-wrapper"><aside id="sidebar-quicklinks" class="sidebar"><button type="button" class="button action backdrop" aria-label="Collapse sidebar"><span class="button-wrap"></span></button><nav aria-label="Related Topics" class="sidebar-inner"><div class="in-nav-toc"><div class="document-toc-container"><section class="document-toc"><header><h2 class="document-toc-heading">In this article</h2></header><ul class="document-toc-list"><li class="document-toc-item "><a class="document-toc-link" href="#audio_graphs">Audio graphs</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_data_whats_in_a_sample">Audio data: what's in a sample</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_buffers_frames_samples_and_channels">Audio buffers: frames, samples, and channels</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_channels">Audio channels</a></li><li class="document-toc-item "><a class="document-toc-link" href="#visualizations">Visualizations</a></li><li class="document-toc-item "><a class="document-toc-link" href="#spatializations">Spatializations</a></li><li class="document-toc-item "><a class="document-toc-link" href="#fan-in_and_fan-out">Fan-in and Fan-out</a></li></ul></section></div></div><div><ol><li><strong><a href="../Web_Audio_API.html">Web Audio API</a></strong></li><li class="toggle"><details open=""><summary>Guides</summary><ol><li><a href="Using_Web_Audio_API.html">Using the Web Audio API</a></li><li><em><a href="Basic_concepts_behind_Web_Audio_API.html" aria-current="page">Basic concepts behind Web Audio API</a></em></li><li><a href="Best_practices.html">Web Audio API best practices</a></li><li><a href="Advanced_techniques.html">Advanced techniques: Creating and sequencing audio</a></li><li><a href="Using_AudioWorklet.html">Background audio processing using AudioWorklet</a></li><li><a href="Controlling_multiple_parameters_with_ConstantSourceNode.html">Controlling multiple parameters with ConstantSourceNode</a></li><li><a href="Migrating_from_webkitAudioContext.html">Migrating from webkitAudioContext</a></li><li><a href="Simple_synth.html">Example and tutorial: Simple synth keyboard</a></li><li><a href="Tools.html">Tools for analyzing Web Audio usage</a></li><li><a href="Using_IIR_filters.html">Using IIR filters</a></li><li><a href="Visualizations_with_Web_Audio_API.html">Visualizations with Web Audio API</a></li><li><a href="Web_audio_spatialization_basics.html">Web audio spatialization basics</a></li></ol></details></li><li class="toggle"><details open=""><summary>Interfaces</summary><ol><li><a href="../AnalyserNode.html"><code>AnalyserNode</code></a></li><li><a href="../AudioBuffer.html"><code>AudioBuffer</code></a></li><li><a href="../AudioBufferSourceNode.html"><code>AudioBufferSourceNode</code></a></li><li><a href="../AudioContext.html"><code>AudioContext</code></a></li><li><a href="../AudioDestinationNode.html"><code>AudioDestinationNode</code></a></li><li><a href="../AudioListener.html"><code>AudioListener</code></a></li><li><a href="../AudioNode.html"><code>AudioNode</code></a></li><li><a href="../AudioParam.html"><code>AudioParam</code></a></li><li><a href="../AudioProcessingEvent.html"><code>AudioProcessingEvent</code></a></li><li><a href="../AudioScheduledSourceNode.html"><code>AudioScheduledSourceNode</code></a></li><li><a href="../AudioSinkInfo.html"><code>AudioSinkInfo</code></a></li><li><a href="../AudioWorklet.html"><code>AudioWorklet</code></a></li><li><a href="../AudioWorkletGlobalScope.html"><code>AudioWorkletGlobalScope</code></a></li><li><a href="../AudioWorkletNode.html"><code>AudioWorkletNode</code></a></li><li><a href="../AudioWorkletProcessor.html"><code>AudioWorkletProcessor</code></a></li><li><a href="../BaseAudioContext.html"><code>BaseAudioContext</code></a></li><li><a href="../BiquadFilterNode.html"><code>BiquadFilterNode</code></a></li><li><a href="../ChannelMergerNode.html"><code>ChannelMergerNode</code></a></li><li><a href="../ChannelSplitterNode.html"><code>ChannelSplitterNode</code></a></li><li><a href="../ConstantSourceNode.html"><code>ConstantSourceNode</code></a></li><li><a href="../ConvolverNode.html"><code>ConvolverNode</code></a></li><li><a href="../DelayNode.html"><code>DelayNode</code></a></li><li><a href="../DynamicsCompressorNode.html"><code>DynamicsCompressorNode</code></a></li><li><a href="../GainNode.html"><code>GainNode</code></a></li><li><a href="../IIRFilterNode.html"><code>IIRFilterNode</code></a></li><li><a href="../MediaElementAudioSourceNode.html"><code>MediaElementAudioSourceNode</code></a></li><li><a href="../MediaStreamAudioDestinationNode.html"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href="../MediaStreamAudioSourceNode.html"><code>MediaStreamAudioSourceNode</code></a></li><li><a href="../OfflineAudioCompletionEvent.html"><code>OfflineAudioCompletionEvent</code></a></li><li><a href="../OfflineAudioContext.html"><code>OfflineAudioContext</code></a></li><li><a href="../OscillatorNode.html"><code>OscillatorNode</code></a></li><li><a href="../PannerNode.html"><code>PannerNode</code></a></li><li><a href="../PeriodicWave.html"><code>PeriodicWave</code></a></li><li><a href="../WaveShaperNode.html"><code>WaveShaperNode</code></a></li><li><a href="../StereoPannerNode.html"><code>StereoPannerNode</code></a></li></ol></details></li></ol></div></nav></aside><aside class="toc"><nav><div class="document-toc-container"><section class="document-toc"><header><h2 class="document-toc-heading">In this article</h2></header><ul class="document-toc-list"><li class="document-toc-item "><a class="document-toc-link" href="#audio_graphs">Audio graphs</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_data_whats_in_a_sample">Audio data: what's in a sample</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_buffers_frames_samples_and_channels">Audio buffers: frames, samples, and channels</a></li><li class="document-toc-item "><a class="document-toc-link" href="#audio_channels">Audio channels</a></li><li class="document-toc-item "><a class="document-toc-link" href="#visualizations">Visualizations</a></li><li class="document-toc-item "><a class="document-toc-link" href="#spatializations">Spatializations</a></li><li class="document-toc-item "><a class="document-toc-link" href="#fan-in_and_fan-out">Fan-in and Fan-out</a></li></ul></section></div></nav></aside><main id="content" class="main-content  "><article class="main-page-content" lang="en-US"><h1>Basic concepts behind Web Audio API</h1><div class="section-content"><p>This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.</p></div><section aria-labelledby="audio_graphs"><a class="dashAnchor" name="//apple_ref/Section/Audio%20graphs"></a><h2 id="audio_graphs"><a href="#audio_graphs">Audio graphs</a></h2><div class="section-content"><p>The <a href="../Web_Audio_API.html">Web Audio API</a> involves handling audio operations inside an <a href="../AudioContext.html">audio context</a>, and has been designed to allow <em>modular routing</em>. Each <a href="../AudioNode.html">audio node</a> performs a basic audio operation and is linked with one more other audio nodes to form an <a href="../AudioNode.html#the_audio_routing_graph">audio routing graph</a>. Several sources with different channel layouts are supported, even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>
<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don't have to provide a destination if you only want to visualize some audio data). A simple, typical workflow for web audio would look something like this:</p>
<ol>
  <li>Create the audio context.</li>
  <li>Create audio sources inside the context (such as <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>, an oscillator, or stream).</li>
  <li>Create audio effects (such as the reverb, biquad filter, panner, or compressor nodes).</li>
  <li>Choose the final destination for the audio (such as the user's computer speakers).</li>
  <li>Connect the source nodes to zero or more effect nodes and then to the chosen destination.</li>
</ol>
<div class="notecard note" id="sect1">
  <p><strong>Note:</strong> The <a href="https://en.wikipedia.org/wiki/Surround_sound#Channel_notation" class="external" target="_blank">channel notation</a> is a numeric value, such as <em>2.0</em> or <em>5.1</em>, representing the number of audio channels available on a signal. The first number is the number of full frequency range audio channels the signal includes. The number appearing after the period indicates the number of those channels reserved for low-frequency effect (LFE) outputs; these are often called <strong>subwoofers</strong>.</p>
</div>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg" alt="A simple box diagram with an outer box labeled Audio context and three inner boxes labeled Sources, Effects, and Destination. The three inner boxes have arrows between them pointing from left to right, indicating the flow of audio information." width="643" height="143" loading="lazy">
</p>
<p>Each input or output is composed of one or more audio <strong>channels</strong>, which together represent a specific audio layout. Any discrete channel structure is supported, including <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, and so on.</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/mdn.png" alt="Show the ability of audio nodes to connect via their inputs and outputs and the channels inside these inputs/outputs." width="1308" height="750" loading="lazy">
</p>
<p>You have several ways to obtain audio:</p>
<ul>
  <li>Sound can be generated directly in JavaScript by an audio node (such as an oscillator).</li>
  <li>It can be created from raw <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation" class="external" target="_blank">PCM</a> data (such as .WAV files or other formats supported by <a href="../BaseAudioContext/decodeAudioData.html" title="decodeAudioData()"><code>decodeAudioData()</code></a>).</li>
  <li>It can be generated from HTML media elements, such as <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video"><code>&lt;video&gt;</code></a> or <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>.</li>
  <li>It can be obtained from a <a href="../WebRTC_API.html">WebRTC</a> <a href="../MediaStream.html"><code>MediaStream</code></a>, such as a webcam or microphone.</li>
</ul></div></section><section aria-labelledby="audio_data_whats_in_a_sample"><a class="dashAnchor" name="//apple_ref/Section/Audio%20data%3A%20what%27s%20in%20a%20sample"></a><h2 id="audio_data_whats_in_a_sample"><a href="#audio_data_whats_in_a_sample">Audio data: what's in a sample</a></h2><div class="section-content"><p>When an audio signal is processed, sampling happens. <strong>Sampling</strong> is the conversion of a <a href="https://en.wikipedia.org/wiki/Continuous_signal" class="external" target="_blank">continuous signal</a> to a <a href="https://en.wikipedia.org/wiki/Discrete_signal" class="external" target="_blank">discrete signal</a>. Put another way, a continuous sound wave, such as a band playing live, is converted into a sequence of digital samples (a discrete-time signal) that allows a computer to handle the audio in distinct blocks.</p>
<p>You'll find more information on the Wikipedia page <a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)" class="external" target="_blank"><em>Sampling (signal processing)</em></a>.</p></div></section><section aria-labelledby="audio_buffers_frames_samples_and_channels"><a class="dashAnchor" name="//apple_ref/Section/Audio%20buffers%3A%20frames%2C%20samples%2C%20and%20channels"></a><h2 id="audio_buffers_frames_samples_and_channels"><a href="#audio_buffers_frames_samples_and_channels">Audio buffers: frames, samples, and channels</a></h2><div class="section-content"><p>An <a href="../AudioBuffer.html"><code>AudioBuffer</code></a> is defined with three parameters:</p>
<ul>
  <li>the number of channels (1 for mono, 2 for stereo, etc.),</li>
  <li>its length, meaning the number of sample frames inside the buffer,</li>
  <li>and the sample rate, the number of sample frames played per second.</li>
</ul>
<p>A <em>sample</em> is a single 32-bit floating point value representing the value of the audio stream at each specific moment in time within a particular channel (left or right, if in the case of stereo). A <em>frame</em>, or <em>sample frame</em>, is the set of all values for all channels that will play at a specific moment in time: all the samples of all the channels that play at the same time (two for a stereo sound, six for 5.1, etc.).</p>
<p>The <em>sample rate</em> is the quantity of those samples (or frames, since all samples of a frame play at the same time) that will play in one second, measured in Hz. The higher the sample rate, the better the sound quality.</p>
<p>Let's look at a <em>mono</em> and a <em>stereo</em> audio buffer, each one second long at a rate of 44100Hz:</p>
<ul>
  <li>The <em>mono</em> buffer will have 44,100 samples and 44,100 frames. The <code>length</code> property will be 44,100.</li>
  <li>The <em>stereo</em> buffer will have 88,200 samples but still 44,100 frames. The <code>length</code> property will still be 44100 since it equals the number of frames.</li>
</ul>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/sampleframe-english.png" alt="A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo." width="1245" height="219" loading="lazy">
</p>
<p>When a buffer plays, you will first hear the leftmost sample frame, then the one right next to it, then the next, <em>and so on</em>, until the end of the buffer. In the case of stereo, you will hear both channels simultaneously. Sample frames are handy because they are independent of the number of channels and represent time in an ideal way for precise audio manipulation.</p>
<div class="notecard note" id="sect2">
  <p><strong>Note:</strong> To get a time in seconds from a frame count, divide the number of frames by the sample rate. To get the number of frames from the number of samples, you only need to divide the latter value by the channel count.</p>
</div>
<p>Here are a couple of simple examples:</p>
<div class="code-example"><pre class="brush: js notranslate"><code><span class="token keyword">const</span> context <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> buffer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioBuffer</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> <span class="token punctuation">{</span>
  <span class="token literal-property property">numberOfChannels</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
  <span class="token literal-property property">length</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
  <span class="token literal-property property">sampleRate</span><span class="token operator">:</span> <span class="token number">44100</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div>
<div class="notecard note" id="sect3">
  <p><strong>Note:</strong> In <a href="https://en.wikipedia.org/wiki/Digital_audio" class="external" target="_blank">digital audio</a>, <strong>44,100 <a href="https://en.wikipedia.org/wiki/Hertz" class="external" target="_blank">Hz</a></strong> (alternately represented as <strong>44.1 kHz</strong>) is a common <a href="https://en.wikipedia.org/wiki/Sampling_frequency" class="external" target="_blank">sampling frequency</a>. Why 44.1 kHz?</p>
  <p>Firstly, because the <a href="https://en.wikipedia.org/wiki/Hearing_range" class="external" target="_blank">hearing range</a> of human ears is roughly 20 Hz to 20,000 Hz. Via the <a href="https://en.wikipedia.org/wiki/Nyquist–Shannon_sampling_theorem" class="external" target="_blank">Nyquist–Shannon sampling theorem</a>, the sampling frequency must be greater than twice the maximum frequency one wishes to reproduce. Therefore, the sampling rate has to be <em>greater</em> than 40,000 Hz.</p>
  <p>Secondly, signals must be <a href="https://en.wikipedia.org/wiki/Low-pass_filter" class="external" target="_blank">low-pass filtered</a> before sampling, otherwise <a href="https://en.wikipedia.org/wiki/Aliasing" class="external" target="_blank">aliasing</a> occurs. While an ideal low-pass filter would perfectly pass frequencies below 20 kHz (without attenuating them) and perfectly cut off frequencies above 20 kHz, in practice, a <a href="https://en.wikipedia.org/wiki/Transition_band" class="external" target="_blank">transition band</a> is necessary, where frequencies are partly attenuated. The wider this transition band is, the easier and more economical it is to make an <a href="https://en.wikipedia.org/wiki/Anti-aliasing_filter" class="external" target="_blank">anti-aliasing filter</a>. The 44.1 kHz sampling frequency allows for a 2.05 kHz transition band.</p>
</div>
<p>If you use this call above, you will get a stereo buffer with two channels that, when played back on an <a href="../AudioContext.html"><code>AudioContext</code></a> running at 44100 Hz (very common, most normal sound cards run at this rate), will last for 0.5 seconds: 22,050 frames/44,100 Hz = 0.5 seconds.</p>
<div class="code-example"><pre class="brush: js notranslate"><code><span class="token keyword">const</span> context <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> buffer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioBuffer</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> <span class="token punctuation">{</span>
  <span class="token literal-property property">numberOfChannels</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token literal-property property">length</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
  <span class="token literal-property property">sampleRate</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div>
<p>If you use this call, you will get a mono buffer (single-channel buffer) that, when played back on an <a href="../AudioContext.html"><code>AudioContext</code></a> running at 44,100 Hz, will be automatically <em>resampled</em> to 44,100 Hz (and therefore yield 44,100 frames), and last for 1.0 second: 44,100 frames/44,100 Hz = 1 second.</p>
<div class="notecard note" id="sect4">
  <p><strong>Note:</strong> Audio resampling is very similar to image resizing. Say you've got a 16 x 16 image but want it to fill a 32 x 32 area. You resize (or resample) it. The result has less quality (it can be blurry or edgy, depending on the resizing algorithm), but it works, with the resized image taking up less space. Resampled audio is the same: you save space, but, in practice, you cannot correctly reproduce high-frequency content or treble sound.</p>
</div></div></section><section aria-labelledby="planar_versus_interleaved_buffers"><h3 id="planar_versus_interleaved_buffers"><a href="#planar_versus_interleaved_buffers">Planar versus interleaved buffers</a></h3><div class="section-content"><p>The Web Audio API uses a planar buffer format. The left and right channels are stored like this:</p>
<pre class="notranslate">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)
</pre>
<p>This structure is widespread in audio processing, making it easy to process each channel independently.</p>
<p>The alternative is to use an interleaved buffer format:</p>
<pre class="notranslate">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)
</pre>
<p>This format is prevalent for storing and playing back audio without much processing, for example: .WAV files or a decoded MP3 stream.</p>
<p>Because the Web Audio API is designed for processing, it exposes <em>only</em> planar buffers. It uses the planar format but converts the audio to the interleaved format when it sends it to the sound card for playback. Conversely, when the API decodes an MP3, it starts with the interleaved format and converts it to the planar format for processing.</p></div></section><section aria-labelledby="audio_channels"><a class="dashAnchor" name="//apple_ref/Section/Audio%20channels"></a><h2 id="audio_channels"><a href="#audio_channels">Audio channels</a></h2><div class="section-content"><p>Each audio buffer may contain different numbers of channels. Most modern audio devices use the basic <em>mono</em> (only one channel) and <em>stereo</em> (left and right channels) settings. Some more complex sets support <em>surround sound</em> settings (like <em>quad</em> and <em>5.1</em>), which can lead to a richer sound experience thanks to their high channel count. We usually represent the channels with the standard abbreviations detailed in the table below:</p>
<figure class="table-container"><table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Channels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Mono</em></td>
      <td><code>0: M: mono</code></td>
    </tr>
    <tr>
      <td><em>Stereo</em></td>
      <td><code>0: L: left 1: R: right</code></td>
    </tr>
    <tr>
      <td><em>Quad</em></td>
      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>
    </tr>
    <tr>
      <td><em>5.1</em></td>
      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>
    </tr>
  </tbody>
</table></figure></div></section><section aria-labelledby="up-mixing_and_down-mixing"><h3 id="up-mixing_and_down-mixing"><a href="#up-mixing_and_down-mixing">Up-mixing and down-mixing</a></h3><div class="section-content"><p>When the numbers of channels of the input and the output don't match, up-mixing, or down-mixing, must be done. The following rules, controlled by setting the <a href="../AudioNode/channelInterpretation.html"><code>AudioNode.channelInterpretation</code></a> property to <code>speakers</code> or <code>discrete</code>, apply:</p>
<figure class="table-container"><table class="standard-table">
  <thead>
    <tr>
      <th scope="row">Interpretation</th>
      <th scope="col">Input channels</th>
      <th scope="col">Output channels</th>
      <th scope="col">Mixing rules</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="13" scope="row"><code>speakers</code></th>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Up-mix from mono to stereo</em>.<br>The <code>M</code> input
        channel is used for both output channels (<code>L</code> and
        <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>
      </td>
    </tr>
    <tr>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Up-mix from mono to quad.</em><br>The <code>M</code> input channel
        is used for non-surround output channels (<code>L</code> and
        <code>R</code>). Surround output channels (<code>SL</code> and
        <code>SR</code>) are silent.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from mono to 5.1.</em><br>The <code>M</code> input channel
        is used for the center output channel (<code>C</code>). All the others
        (<code>L</code>, <code>R</code>, <code>LFE</code>, <code>SL</code>, and
        <code>SR</code>) are silent.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from stereo to mono</em>.<br>Both input channels (<code>L</code>
        and <code>R</code>) are equally combined to produce the unique output
        channel (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Up-mix from stereo to quad.</em><br>The <code>L</code> and
        <code>R </code>input channels are used for their non-surround respective
        output channels (<code>L</code> and <code>R</code>). Surround output
        channels (<code>SL</code> and <code>SR</code>) are silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from stereo to 5.1.</em><br>The <code>L</code> and
        <code>R </code>input channels are used for their non-surround respective
        output channels (<code>L</code> and <code>R</code>). Surround output
        channels (<code>SL</code> and <code>SR</code>), as well as the center
        (<code>C</code>) and subwoofer (<code>LFE</code>) channels, are left
        silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE
= 0<br>output.SL = 0<br>output.SR = 0</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from quad to mono</em>.<br>All four input channels
        (<code>L</code>, <code>R</code>, <code>SL</code>, and <code>SR</code>)
        are equally combined to produce the unique output channel
        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Down-mix from quad to stereo</em>.<br>Both left input channels
        (<code>L</code> and <code>SL</code>) are equally combined to produce the
        unique left output channel (<code>L</code>). And similarly, both right
        input channels (<code>R</code> and <code>SR</code>) are equally combined
        to produce the unique right output channel (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from quad to 5.1.</em><br>The <code>L</code>,
        <code>R</code>, <code>SL</code>, and <code>SR</code> input channels are
        used for their respective output channels (<code>L</code> and
        <code>R</code>). Center (<code>C</code>) and subwoofer
        (<code>LFE</code>) channels are left silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE
= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from 5.1 to mono.</em><br>The left (<code>L</code> and
        <code>SL</code>), right (<code>R</code> and <code>SR</code>) and central
        channels are all mixed together. The surround channels are slightly
        attenuated, and the regular lateral channels are power-compensated to
        make them count as a single channel by multiplying by <code>√2/2</code>.
        The subwoofer (<code>LFE</code>) channel is lost.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +
input.SR)</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Down-mix from 5.1 to stereo.</em><br>The central channel
        (<code>C</code>) is summed with each lateral surround channel (<code>SL</code>
        or <code>SR</code>) and mixed to each lateral channel. As it is mixed
        down to two channels, it is mixed at a lower power: in each case, it is
        multiplied by <code>√2/2</code>. The subwoofer (<code>LFE</code>)
        channel is lost.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =
input.R </code><code>+ 0.7071 * (input.C + input.SR)</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Down-mix from 5.1 to quad.</em><br>The central (<code>C</code>) is
        mixed with the lateral non-surround channels (<code>L</code> and
        <code>R</code>). As it is mixed down to two channels, it is mixed at a
        lower power: in each case, it is multiplied by <code>√2/2</code>. The
        surround channels are passed unchanged. The subwoofer (<code>LFE</code>)
        channel is lost.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +
0.7071 * input.C<br>output.SL = input.SL<br>output.SR =
input.SR</code>
      </td>
    </tr>
    <tr>
      <td colspan="2">Other, non-standard layouts</td>
      <td>
        Non-standard channel layouts behave as if
        <code>channelInterpretation</code> is set to
        <code>discrete</code>.<br>The specification explicitly allows the future definition of new speaker layouts. Therefore, this fallback is not future-proof as the behavior of the browsers for a specific number of channels may change in the future.
      </td>
    </tr>
    <tr>
      <th rowspan="2" scope="row"><code>discrete</code></th>
      <td>any (<code>x</code>)</td>
      <td>any (<code>y</code>) where <code>x&lt;y</code></td>
      <td>
        <em>Up-mix discrete channels.</em><br>Fill each output channel with
        its input counterpart — that is, the input channel with the same index.
        Channels with no corresponding input channels are left silent.
      </td>
    </tr>
    <tr>
      <td>any (<code>x</code>)</td>
      <td>any (<code>y</code>) where <code>x&gt;y</code></td>
      <td>
        <em>Down-mix discrete channels.</em><br>Fill each output channel with
        its input counterpart — that is, the input channel with the same index.
        Input channels with no corresponding output channels are dropped.
      </td>
    </tr>
  </tbody>
</table></figure></div></section><section aria-labelledby="visualizations"><a class="dashAnchor" name="//apple_ref/Section/Visualizations"></a><h2 id="visualizations"><a href="#visualizations">Visualizations</a></h2><div class="section-content"><p>In general, we get the output over time to produce audio visualizations, usually reading its gain or frequency data. Then, using a graphical tool, we turn the obtained data into a visual representation, such as a graph. The Web Audio API has an <a href="../AnalyserNode.html"><code>AnalyserNode</code></a> available that doesn't alter the audio signal passing through it. Additionally, it outputs the audio data, allowing us to process it via a technology such as <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/canvas"><code>&lt;canvas&gt;</code></a>.</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg" alt="Without modifying the audio stream, the node allows to get the frequency and time-domain data associated with it, using an FFT." width="693" height="206" loading="lazy">
</p>
<p>You can grab data using the following methods:</p>
<dl>
  <dt id="analysernode.getfloatfrequencydata"><a href="../AnalyserNode/getFloatFrequencyData.html"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>
  <dd>
    <p>Copies the current frequency data into a <a href="../../JavaScript/Reference/Global_Objects/Float32Array.html"><code>Float32Array</code></a> array passed into it.</p>
  </dd>
  <dt id="analysernode.getbytefrequencydata"><a href="../AnalyserNode/getByteFrequencyData.html"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>
  <dd>
    <p>Copies the current frequency data into a <a href="../../JavaScript/Reference/Global_Objects/Uint8Array.html"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>
  </dd>
  <dt id="analysernode.getfloattimedomaindata"><a href="../AnalyserNode/getFloatTimeDomainData.html"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>
  <dd>
    <p>Copies the current waveform, or time-domain, data into a <a href="../../JavaScript/Reference/Global_Objects/Float32Array.html"><code>Float32Array</code></a> array passed into it.</p>
  </dd>
  <dt id="analysernode.getbytetimedomaindata"><a href="../AnalyserNode/getByteTimeDomainData.html"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>
  <dd>
    <p>Copies the current waveform, or time-domain, data into a <a href="../../JavaScript/Reference/Global_Objects/Uint8Array.html"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>
  </dd>
</dl>
<div class="notecard note" id="sect5">
  <p><strong>Note:</strong> For more information, see our <a href="Visualizations_with_Web_Audio_API.html">Visualizations with Web Audio API</a> article.</p>
</div></div></section><section aria-labelledby="spatializations"><a class="dashAnchor" name="//apple_ref/Section/Spatializations"></a><h2 id="spatializations"><a href="#spatializations">Spatializations</a></h2><div class="section-content"><p>Audio spatialization allows us to model the position and behavior of an audio signal at a certain point in physical space, simulating the listener hearing that audio. In the Web Audio API, spatialization is handled by the <a href="../PannerNode.html"><code>PannerNode</code></a> and the <a href="../AudioListener.html"><code>AudioListener</code></a>.</p>
<p>The panner uses right-hand Cartesian coordinates to describe the audio source's <em>position</em> as a vector and its <em>orientation</em> as a 3D directional cone. The cone can be pretty large, for example, for omnidirectional sources.</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/pannernode_en.svg" alt="The PannerNode defines a spatial position and orientation for a given signal." width="799" height="340" loading="lazy">
</p>
<p>Similarly, the Web Audio API describes the listener using right-hand Cartesian coordinates: their <em>position</em> as one vector and their <em>orientation</em> as two direction vectors, <em>up</em> and <em>front</em>. These vectors define the direction of the top of the listener's head and the direction the listener's nose is pointing. The vectors are perpendicular to one another.</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/webaudiolistenerreduced.png" alt="We see the position, up, and front vectors of an AudioListener, with the up and front vectors at 90° from the other." width="634" height="250" loading="lazy">
</p>
<div class="notecard note" id="sect6">
  <p><strong>Note:</strong> For more information, see our <a href="Web_audio_spatialization_basics.html">Web audio spatialization basics</a> article.</p>
</div></div></section><section aria-labelledby="fan-in_and_fan-out"><a class="dashAnchor" name="//apple_ref/Section/Fan%2Din%20and%20Fan%2Dout"></a><h2 id="fan-in_and_fan-out"><a href="#fan-in_and_fan-out">Fan-in and Fan-out</a></h2><div class="section-content"><p>In audio terms, <strong>fan-in</strong> describes the process by which a <a href="../ChannelMergerNode.html"><code>ChannelMergerNode</code></a> takes a series of <em>mono</em> input sources and outputs a single multi-channel signal:</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/fanin.svg" alt="Fan-in process diagram. Multiple point-less arrows representing mono-input sources combine to output a single pointed arrow representing a single multi-channel signal" width="325" height="258" loading="lazy">
</p>
<p><strong>Fan-out</strong> describes the opposite process, whereby a <a href="../ChannelSplitterNode.html"><code>ChannelSplitterNode</code></a> takes a multi-channel input source and outputs multiple <em>mono</em> output signals:</p>
<p>
  <img src="Basic_concepts_behind_Web_Audio_API/fanout.svg" alt="Fan-out process diagram. A single point-less arrow representing a multi-channel input source splits to output multiple pointed arrows representing multiple mono output signals" width="325" height="258" loading="lazy">
</p></div></section><aside class="metadata"><div class="metadata-content-container"><div id="on-github" class="on-github"><h3>Found a content problem with this page?</h3><ul><li>Edit the page <a href="https://github.com/mdn/content/edit/main/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md" title="This will take you to GitHub, where you'll need to sign in first." target="_blank" rel="noopener noreferrer">on GitHub</a>.</li><li>Report the <a href="https://github.com/mdn/content/issues/new?template=page-report.yml&amp;mdn-url=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API%2FBasic_concepts_behind_Web_Audio_API&amp;metadata=%3C%21--+Do+not+make+changes+below+this+line+--%3E%0A%3Cdetails%3E%0A%3Csummary%3EPage+report+details%3C%2Fsummary%3E%0A%0A*+Folder%3A+%60en-us%2Fweb%2Fapi%2Fweb_audio_api%2Fbasic_concepts_behind_web_audio_api%60%0A*+MDN+URL%3A+https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API%2FBasic_concepts_behind_Web_Audio_API%0A*+GitHub+URL%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fblob%2Fmain%2Ffiles%2Fen-us%2Fweb%2Fapi%2Fweb_audio_api%2Fbasic_concepts_behind_web_audio_api%2Findex.md%0A*+Last+commit%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fcommit%2F9791b44f8261f970e47271edcdc932ad6dc63878%0A*+Document+last+modified%3A+2022-10-28T03%3A38%3A24.000Z%0A%0A%3C%2Fdetails%3E" title="This will take you to GitHub to file a new issue." target="_blank" rel="noopener noreferrer">content issue</a>.</li><li>View the source <a href="https://github.com/mdn/content/blob/main/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md?plain=1" title="Folder: en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api (Opens in a new tab)" target="_blank" rel="noopener noreferrer">on GitHub</a>.</li></ul>Want to get more involved? Learn<!-- --> <a href="https://github.com/mdn/content/blob/main/CONTRIBUTING.md" title="This will take you to our contribution guidelines on GitHub." target="_blank" rel="noopener noreferrer">how to contribute</a>.</div><p class="last-modified-date">This page was last modified on<!-- --> <time datetime="2022-10-28T03:38:24.000Z">Oct 28, 2022</time> by<!-- --> <a href="Basic_concepts_behind_Web_Audio_API/contributors.txt">MDN contributors</a>.</p></div></aside></article></main></div><footer id="nav-footer" class="page-footer"><div><a href="http://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a> by <a href="http://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API$history">Mozilla Contributors</a> is licensed under <a href="http://creativecommons.org/licenses/by-sa/2.5/">CC-BY-SA 2.5</a>.</div></footer></div></div><script type="application/json" id="hydration">{"doc":{"isMarkdown":true,"isTranslated":false,"isActive":true,"flaws":{},"title":"Basic concepts behind Web Audio API","mdn_url":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","locale":"en-US","native":"English (US)","sidebarHTML":"<ol><li><strong><a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a></strong></li><li class=\"toggle\"><details open=\"\"><summary>Guides</summary><ol><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API\">Using the Web Audio API</a></li><li><em><a href=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API\" aria-current=\"page\">Basic concepts behind Web Audio API</a></em></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Best_practices\">Web Audio API best practices</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques\">Advanced techniques: Creating and sequencing audio</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet\">Background audio processing using AudioWorklet</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode\">Controlling multiple parameters with ConstantSourceNode</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Migrating_from_webkitAudioContext\">Migrating from webkitAudioContext</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Simple_synth\">Example and tutorial: Simple synth keyboard</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Tools\">Tools for analyzing Web Audio usage</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters\">Using IIR filters</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Interfaces</summary><ol><li><a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioDestinationNode\"><code>AudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioNode\"><code>AudioNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioParam\"><code>AudioParam</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioProcessingEvent\"><code>AudioProcessingEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioScheduledSourceNode\"><code>AudioScheduledSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioSinkInfo\"><code>AudioSinkInfo</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorklet\"><code>AudioWorklet</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletGlobalScope\"><code>AudioWorkletGlobalScope</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletNode\"><code>AudioWorkletNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletProcessor\"><code>AudioWorkletProcessor</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext\"><code>BaseAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/BiquadFilterNode\"><code>BiquadFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConstantSourceNode\"><code>ConstantSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConvolverNode\"><code>ConvolverNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DelayNode\"><code>DelayNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DynamicsCompressorNode\"><code>DynamicsCompressorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/GainNode\"><code>GainNode</code></a></li><li><a href=\"/en-US/docs/Web/API/IIRFilterNode\"><code>IIRFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaElementAudioSourceNode\"><code>MediaElementAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioDestinationNode\"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioSourceNode\"><code>MediaStreamAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioCompletionEvent\"><code>OfflineAudioCompletionEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PeriodicWave\"><code>PeriodicWave</code></a></li><li><a href=\"/en-US/docs/Web/API/WaveShaperNode\"><code>WaveShaperNode</code></a></li><li><a href=\"/en-US/docs/Web/API/StereoPannerNode\"><code>StereoPannerNode</code></a></li></ol></details></li></ol>","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.</p>"}},{"type":"prose","value":{"id":"audio_graphs","title":"Audio graphs","isH3":false,"content":"<p>The <a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a> involves handling audio operations inside an <a href=\"/en-US/docs/Web/API/AudioContext\">audio context</a>, and has been designed to allow <em>modular routing</em>. Each <a href=\"/en-US/docs/Web/API/AudioNode\">audio node</a> performs a basic audio operation and is linked with one more other audio nodes to form an <a href=\"/en-US/docs/Web/API/AudioNode#the_audio_routing_graph\">audio routing graph</a>. Several sources with different channel layouts are supported, even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>\n<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don't have to provide a destination if you only want to visualize some audio data). A simple, typical workflow for web audio would look something like this:</p>\n<ol>\n  <li>Create the audio context.</li>\n  <li>Create audio sources inside the context (such as <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>, an oscillator, or stream).</li>\n  <li>Create audio effects (such as the reverb, biquad filter, panner, or compressor nodes).</li>\n  <li>Choose the final destination for the audio (such as the user's computer speakers).</li>\n  <li>Connect the source nodes to zero or more effect nodes and then to the chosen destination.</li>\n</ol>\n<div class=\"notecard note\" id=\"sect1\">\n  <p><strong>Note:</strong> The <a href=\"https://en.wikipedia.org/wiki/Surround_sound#Channel_notation\" class=\"external\" target=\"_blank\">channel notation</a> is a numeric value, such as <em>2.0</em> or <em>5.1</em>, representing the number of audio channels available on a signal. The first number is the number of full frequency range audio channels the signal includes. The number appearing after the period indicates the number of those channels reserved for low-frequency effect (LFE) outputs; these are often called <strong>subwoofers</strong>.</p>\n</div>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg\" alt=\"A simple box diagram with an outer box labeled Audio context and three inner boxes labeled Sources, Effects, and Destination. The three inner boxes have arrows between them pointing from left to right, indicating the flow of audio information.\" width=\"643\" height=\"143\" loading=\"lazy\">\n</p>\n<p>Each input or output is composed of one or more audio <strong>channels</strong>, which together represent a specific audio layout. Any discrete channel structure is supported, including <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, and so on.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/mdn.png\" alt=\"Show the ability of audio nodes to connect via their inputs and outputs and the channels inside these inputs/outputs.\" width=\"1308\" height=\"750\" loading=\"lazy\">\n</p>\n<p>You have several ways to obtain audio:</p>\n<ul>\n  <li>Sound can be generated directly in JavaScript by an audio node (such as an oscillator).</li>\n  <li>It can be created from raw <a href=\"https://en.wikipedia.org/wiki/Pulse-code_modulation\" class=\"external\" target=\"_blank\">PCM</a> data (such as .WAV files or other formats supported by <a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\" title=\"decodeAudioData()\"><code>decodeAudioData()</code></a>).</li>\n  <li>It can be generated from HTML media elements, such as <a href=\"/en-US/docs/Web/HTML/Element/video\"><code>&lt;video&gt;</code></a> or <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>.</li>\n  <li>It can be obtained from a <a href=\"/en-US/docs/Web/API/WebRTC_API\">WebRTC</a> <a href=\"/en-US/docs/Web/API/MediaStream\"><code>MediaStream</code></a>, such as a webcam or microphone.</li>\n</ul>"}},{"type":"prose","value":{"id":"audio_data_whats_in_a_sample","title":"Audio data: what's in a sample","isH3":false,"content":"<p>When an audio signal is processed, sampling happens. <strong>Sampling</strong> is the conversion of a <a href=\"https://en.wikipedia.org/wiki/Continuous_signal\" class=\"external\" target=\"_blank\">continuous signal</a> to a <a href=\"https://en.wikipedia.org/wiki/Discrete_signal\" class=\"external\" target=\"_blank\">discrete signal</a>. Put another way, a continuous sound wave, such as a band playing live, is converted into a sequence of digital samples (a discrete-time signal) that allows a computer to handle the audio in distinct blocks.</p>\n<p>You'll find more information on the Wikipedia page <a href=\"https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29\" class=\"external\" target=\"_blank\"><em>Sampling (signal processing)</em></a>.</p>"}},{"type":"prose","value":{"id":"audio_buffers_frames_samples_and_channels","title":"Audio buffers: frames, samples, and channels","isH3":false,"content":"<p>An <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> is defined with three parameters:</p>\n<ul>\n  <li>the number of channels (1 for mono, 2 for stereo, etc.),</li>\n  <li>its length, meaning the number of sample frames inside the buffer,</li>\n  <li>and the sample rate, the number of sample frames played per second.</li>\n</ul>\n<p>A <em>sample</em> is a single 32-bit floating point value representing the value of the audio stream at each specific moment in time within a particular channel (left or right, if in the case of stereo). A <em>frame</em>, or <em>sample frame</em>, is the set of all values for all channels that will play at a specific moment in time: all the samples of all the channels that play at the same time (two for a stereo sound, six for 5.1, etc.).</p>\n<p>The <em>sample rate</em> is the quantity of those samples (or frames, since all samples of a frame play at the same time) that will play in one second, measured in Hz. The higher the sample rate, the better the sound quality.</p>\n<p>Let's look at a <em>mono</em> and a <em>stereo</em> audio buffer, each one second long at a rate of 44100Hz:</p>\n<ul>\n  <li>The <em>mono</em> buffer will have 44,100 samples and 44,100 frames. The <code>length</code> property will be 44,100.</li>\n  <li>The <em>stereo</em> buffer will have 88,200 samples but still 44,100 frames. The <code>length</code> property will still be 44100 since it equals the number of frames.</li>\n</ul>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe-english.png\" alt=\"A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo.\" width=\"1245\" height=\"219\" loading=\"lazy\">\n</p>\n<p>When a buffer plays, you will first hear the leftmost sample frame, then the one right next to it, then the next, <em>and so on</em>, until the end of the buffer. In the case of stereo, you will hear both channels simultaneously. Sample frames are handy because they are independent of the number of channels and represent time in an ideal way for precise audio manipulation.</p>\n<div class=\"notecard note\" id=\"sect2\">\n  <p><strong>Note:</strong> To get a time in seconds from a frame count, divide the number of frames by the sample rate. To get the number of frames from the number of samples, you only need to divide the latter value by the channel count.</p>\n</div>\n<p>Here are a couple of simple examples:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">44100</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<div class=\"notecard note\" id=\"sect3\">\n  <p><strong>Note:</strong> In <a href=\"https://en.wikipedia.org/wiki/Digital_audio\" class=\"external\" target=\"_blank\">digital audio</a>, <strong>44,100 <a href=\"https://en.wikipedia.org/wiki/Hertz\" class=\"external\" target=\"_blank\">Hz</a></strong> (alternately represented as <strong>44.1 kHz</strong>) is a common <a href=\"https://en.wikipedia.org/wiki/Sampling_frequency\" class=\"external\" target=\"_blank\">sampling frequency</a>. Why 44.1 kHz?</p>\n  <p>Firstly, because the <a href=\"https://en.wikipedia.org/wiki/Hearing_range\" class=\"external\" target=\"_blank\">hearing range</a> of human ears is roughly 20 Hz to 20,000 Hz. Via the <a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\" class=\"external\" target=\"_blank\">Nyquist–Shannon sampling theorem</a>, the sampling frequency must be greater than twice the maximum frequency one wishes to reproduce. Therefore, the sampling rate has to be <em>greater</em> than 40,000 Hz.</p>\n  <p>Secondly, signals must be <a href=\"https://en.wikipedia.org/wiki/Low-pass_filter\" class=\"external\" target=\"_blank\">low-pass filtered</a> before sampling, otherwise <a href=\"https://en.wikipedia.org/wiki/Aliasing\" class=\"external\" target=\"_blank\">aliasing</a> occurs. While an ideal low-pass filter would perfectly pass frequencies below 20 kHz (without attenuating them) and perfectly cut off frequencies above 20 kHz, in practice, a <a href=\"https://en.wikipedia.org/wiki/Transition_band\" class=\"external\" target=\"_blank\">transition band</a> is necessary, where frequencies are partly attenuated. The wider this transition band is, the easier and more economical it is to make an <a href=\"https://en.wikipedia.org/wiki/Anti-aliasing_filter\" class=\"external\" target=\"_blank\">anti-aliasing filter</a>. The 44.1 kHz sampling frequency allows for a 2.05 kHz transition band.</p>\n</div>\n<p>If you use this call above, you will get a stereo buffer with two channels that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44100 Hz (very common, most normal sound cards run at this rate), will last for 0.5 seconds: 22,050 frames/44,100 Hz = 0.5 seconds.</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>If you use this call, you will get a mono buffer (single-channel buffer) that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44,100 Hz, will be automatically <em>resampled</em> to 44,100 Hz (and therefore yield 44,100 frames), and last for 1.0 second: 44,100 frames/44,100 Hz = 1 second.</p>\n<div class=\"notecard note\" id=\"sect4\">\n  <p><strong>Note:</strong> Audio resampling is very similar to image resizing. Say you've got a 16 x 16 image but want it to fill a 32 x 32 area. You resize (or resample) it. The result has less quality (it can be blurry or edgy, depending on the resizing algorithm), but it works, with the resized image taking up less space. Resampled audio is the same: you save space, but, in practice, you cannot correctly reproduce high-frequency content or treble sound.</p>\n</div>"}},{"type":"prose","value":{"id":"planar_versus_interleaved_buffers","title":"Planar versus interleaved buffers","isH3":true,"content":"<p>The Web Audio API uses a planar buffer format. The left and right channels are stored like this:</p>\n<pre class=\"notranslate\">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)\n</pre>\n<p>This structure is widespread in audio processing, making it easy to process each channel independently.</p>\n<p>The alternative is to use an interleaved buffer format:</p>\n<pre class=\"notranslate\">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)\n</pre>\n<p>This format is prevalent for storing and playing back audio without much processing, for example: .WAV files or a decoded MP3 stream.</p>\n<p>Because the Web Audio API is designed for processing, it exposes <em>only</em> planar buffers. It uses the planar format but converts the audio to the interleaved format when it sends it to the sound card for playback. Conversely, when the API decodes an MP3, it starts with the interleaved format and converts it to the planar format for processing.</p>"}},{"type":"prose","value":{"id":"audio_channels","title":"Audio channels","isH3":false,"content":"<p>Each audio buffer may contain different numbers of channels. Most modern audio devices use the basic <em>mono</em> (only one channel) and <em>stereo</em> (left and right channels) settings. Some more complex sets support <em>surround sound</em> settings (like <em>quad</em> and <em>5.1</em>), which can lead to a richer sound experience thanks to their high channel count. We usually represent the channels with the standard abbreviations detailed in the table below:</p>\n<figure class=\"table-container\"><table>\n  <thead>\n    <tr>\n      <th>Name</th>\n      <th>Channels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><em>Mono</em></td>\n      <td><code>0: M: mono</code></td>\n    </tr>\n    <tr>\n      <td><em>Stereo</em></td>\n      <td><code>0: L: left 1: R: right</code></td>\n    </tr>\n    <tr>\n      <td><em>Quad</em></td>\n      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>\n    </tr>\n    <tr>\n      <td><em>5.1</em></td>\n      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>\n    </tr>\n  </tbody>\n</table></figure>"}},{"type":"prose","value":{"id":"up-mixing_and_down-mixing","title":"Up-mixing and down-mixing","isH3":true,"content":"<p>When the numbers of channels of the input and the output don't match, up-mixing, or down-mixing, must be done. The following rules, controlled by setting the <a href=\"/en-US/docs/Web/API/AudioNode/channelInterpretation\"><code>AudioNode.channelInterpretation</code></a> property to <code>speakers</code> or <code>discrete</code>, apply:</p>\n<figure class=\"table-container\"><table class=\"standard-table\">\n  <thead>\n    <tr>\n      <th scope=\"row\">Interpretation</th>\n      <th scope=\"col\">Input channels</th>\n      <th scope=\"col\">Output channels</th>\n      <th scope=\"col\">Mixing rules</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"13\" scope=\"row\"><code>speakers</code></th>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Up-mix from mono to stereo</em>.<br>The <code>M</code> input\n        channel is used for both output channels (<code>L</code> and\n        <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from mono to quad.</em><br>The <code>M</code> input channel\n        is used for non-surround output channels (<code>L</code> and\n        <code>R</code>). Surround output channels (<code>SL</code> and\n        <code>SR</code>) are silent.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from mono to 5.1.</em><br>The <code>M</code> input channel\n        is used for the center output channel (<code>C</code>). All the others\n        (<code>L</code>, <code>R</code>, <code>LFE</code>, <code>SL</code>, and\n        <code>SR</code>) are silent.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from stereo to mono</em>.<br>Both input channels (<code>L</code>\n        and <code>R</code>) are equally combined to produce the unique output\n        channel (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from stereo to quad.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>) are silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from stereo to 5.1.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>), as well as the center\n        (<code>C</code>) and subwoofer (<code>LFE</code>) channels, are left\n        silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = 0<br>output.SR = 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from quad to mono</em>.<br>All four input channels\n        (<code>L</code>, <code>R</code>, <code>SL</code>, and <code>SR</code>)\n        are equally combined to produce the unique output channel\n        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from quad to stereo</em>.<br>Both left input channels\n        (<code>L</code> and <code>SL</code>) are equally combined to produce the\n        unique left output channel (<code>L</code>). And similarly, both right\n        input channels (<code>R</code> and <code>SR</code>) are equally combined\n        to produce the unique right output channel (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from quad to 5.1.</em><br>The <code>L</code>,\n        <code>R</code>, <code>SL</code>, and <code>SR</code> input channels are\n        used for their respective output channels (<code>L</code> and\n        <code>R</code>). Center (<code>C</code>) and subwoofer\n        (<code>LFE</code>) channels are left silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to mono.</em><br>The left (<code>L</code> and\n        <code>SL</code>), right (<code>R</code> and <code>SR</code>) and central\n        channels are all mixed together. The surround channels are slightly\n        attenuated, and the regular lateral channels are power-compensated to\n        make them count as a single channel by multiplying by <code>√2/2</code>.\n        The subwoofer (<code>LFE</code>) channel is lost.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +\ninput.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to stereo.</em><br>The central channel\n        (<code>C</code>) is summed with each lateral surround channel (<code>SL</code>\n        or <code>SR</code>) and mixed to each lateral channel. As it is mixed\n        down to two channels, it is mixed at a lower power: in each case, it is\n        multiplied by <code>√2/2</code>. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =\ninput.R </code><code>+ 0.7071 * (input.C + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to quad.</em><br>The central (<code>C</code>) is\n        mixed with the lateral non-surround channels (<code>L</code> and\n        <code>R</code>). As it is mixed down to two channels, it is mixed at a\n        lower power: in each case, it is multiplied by <code>√2/2</code>. The\n        surround channels are passed unchanged. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +\n0.7071 * input.C<br>output.SL = input.SL<br>output.SR =\ninput.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td colspan=\"2\">Other, non-standard layouts</td>\n      <td>\n        Non-standard channel layouts behave as if\n        <code>channelInterpretation</code> is set to\n        <code>discrete</code>.<br>The specification explicitly allows the future definition of new speaker layouts. Therefore, this fallback is not future-proof as the behavior of the browsers for a specific number of channels may change in the future.\n      </td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" scope=\"row\"><code>discrete</code></th>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&lt;y</code></td>\n      <td>\n        <em>Up-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart — that is, the input channel with the same index.\n        Channels with no corresponding input channels are left silent.\n      </td>\n    </tr>\n    <tr>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&gt;y</code></td>\n      <td>\n        <em>Down-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart — that is, the input channel with the same index.\n        Input channels with no corresponding output channels are dropped.\n      </td>\n    </tr>\n  </tbody>\n</table></figure>"}},{"type":"prose","value":{"id":"visualizations","title":"Visualizations","isH3":false,"content":"<p>In general, we get the output over time to produce audio visualizations, usually reading its gain or frequency data. Then, using a graphical tool, we turn the obtained data into a visual representation, such as a graph. The Web Audio API has an <a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a> available that doesn't alter the audio signal passing through it. Additionally, it outputs the audio data, allowing us to process it via a technology such as <a href=\"/en-US/docs/Web/HTML/Element/canvas\"><code>&lt;canvas&gt;</code></a>.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg\" alt=\"Without modifying the audio stream, the node allows to get the frequency and time-domain data associated with it, using an FFT.\" width=\"693\" height=\"206\" loading=\"lazy\">\n</p>\n<p>You can grab data using the following methods:</p>\n<dl>\n  <dt id=\"analysernode.getfloatfrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData\"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytefrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getfloattimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData\"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytetimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData\"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n</dl>\n<div class=\"notecard note\" id=\"sect5\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"spatializations","title":"Spatializations","isH3":false,"content":"<p>Audio spatialization allows us to model the position and behavior of an audio signal at a certain point in physical space, simulating the listener hearing that audio. In the Web Audio API, spatialization is handled by the <a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a> and the <a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a>.</p>\n<p>The panner uses right-hand Cartesian coordinates to describe the audio source's <em>position</em> as a vector and its <em>orientation</em> as a 3D directional cone. The cone can be pretty large, for example, for omnidirectional sources.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode_en.svg\" alt=\"The PannerNode defines a spatial position and orientation for a given signal.\" width=\"799\" height=\"340\" loading=\"lazy\">\n</p>\n<p>Similarly, the Web Audio API describes the listener using right-hand Cartesian coordinates: their <em>position</em> as one vector and their <em>orientation</em> as two direction vectors, <em>up</em> and <em>front</em>. These vectors define the direction of the top of the listener's head and the direction the listener's nose is pointing. The vectors are perpendicular to one another.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudiolistenerreduced.png\" alt=\"We see the position, up, and front vectors of an AudioListener, with the up and front vectors at 90° from the other.\" width=\"634\" height=\"250\" loading=\"lazy\">\n</p>\n<div class=\"notecard note\" id=\"sect6\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"fan-in_and_fan-out","title":"Fan-in and Fan-out","isH3":false,"content":"<p>In audio terms, <strong>fan-in</strong> describes the process by which a <a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a> takes a series of <em>mono</em> input sources and outputs a single multi-channel signal:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg\" alt=\"Fan-in process diagram. Multiple point-less arrows representing mono-input sources combine to output a single pointed arrow representing a single multi-channel signal\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>\n<p><strong>Fan-out</strong> describes the opposite process, whereby a <a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a> takes a multi-channel input source and outputs multiple <em>mono</em> output signals:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg\" alt=\"Fan-out process diagram. A single point-less arrow representing a multi-channel input source splits to output multiple pointed arrows representing multiple mono output signals\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>"}}],"toc":[{"text":"Audio graphs","id":"audio_graphs"},{"text":"Audio data: what's in a sample","id":"audio_data_whats_in_a_sample"},{"text":"Audio buffers: frames, samples, and channels","id":"audio_buffers_frames_samples_and_channels"},{"text":"Audio channels","id":"audio_channels"},{"text":"Visualizations","id":"visualizations"},{"text":"Spatializations","id":"spatializations"},{"text":"Fan-in and Fan-out","id":"fan-in_and_fan-out"}],"summary":"This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.","popularity":0,"modified":"2022-10-28T03:38:24.000Z","other_translations":[{"title":"Les concepts de base de la Web Audio API","locale":"fr","native":"Français"},{"title":"Basic concepts behind Web Audio API","locale":"ja","native":"日本語"},{"title":"Web Audio API의 기본 개념","locale":"ko","native":"한국어"},{"title":"网页音频接口的基本概念","locale":"zh-CN","native":"中文 (简体)"}],"source":{"folder":"en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api","github_url":"https://github.com/mdn/content/blob/main/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md","last_commit_url":"https://github.com/mdn/content/commit/9791b44f8261f970e47271edcdc932ad6dc63878","filename":"index.md"},"short_title":"Basic concepts behind Web Audio API","parents":[{"uri":"/en-US/docs/Web","title":"References"},{"uri":"/en-US/docs/Web/API","title":"Web APIs"},{"uri":"/en-US/docs/Web/API/Web_Audio_API","title":"Web Audio API"},{"uri":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","title":"Basic concepts behind Web Audio API"}],"pageTitle":"Basic concepts behind Web Audio API - Web APIs | MDN","noIndexing":false}}</script>
<!-- Mirrored from developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 14 Feb 2023 04:49:12 GMT -->
</body></html>